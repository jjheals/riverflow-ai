{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook walks through the segmentation process for images.\n",
        "\n",
        "*Note that this was developed using Google Colab and does not neatly fit within the rest of the project structure as is; this notebook is meant to show the process, not necessarily a universal implementation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVn64Qs3gr8T"
      },
      "source": [
        "The functionaliuty of the Colab notebook is used to access a drive folder with the cleaned dat to be segmented. Video files are parsed from there and then compared with what already exists so as not to repeat segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFhLhP23tic-",
        "outputId": "c46425e5-b034-454d-d06d-6edf0a1e447b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder = \"/content/drive/MyDrive/data/CleanedData\"\n",
        "target_folder = \"/content/drive/MyDrive/data/SegmentedData2\"\n",
        "\n",
        "if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)      \n",
        "\n",
        "all_files = [f for f in os.listdir(folder) if '.' in f]\n",
        "is_video = lambda f : f.endswith('avi')\n",
        "video_files = [f for f in all_files if is_video(f)]\n",
        "print(len(video_files))\n",
        "completed_videos = [f for f in os.listdir(target_folder) if '.' in f]\n",
        "video_files = [f for f in video_files if f not in completed_videos or True]\n",
        "print(f\"Total Videos to Process: {len(video_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Function definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "R8GucSeb711y",
        "outputId": "04b21a98-f7d5-4c93-fa7b-819417ff8891"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import slic\n",
        "\n",
        "\n",
        "def mask_segmented_image(binary_image):\n",
        "    gray = cv.cvtColor(binary_image, cv.COLOR_BGR2GRAY)\n",
        "    unique_colors, counts = np.unique(gray, return_counts=True)\n",
        "    color1, color2 = unique_colors\n",
        "    count1, count2 = counts\n",
        "    more_common_color = color1 if count1 > count2 else color2\n",
        "    result_image = np.where(gray == more_common_color, 0, 1).astype(np.uint8) * 255\n",
        "    result_image = cv.cvtColor(result_image, cv.COLOR_GRAY2BGR)\n",
        "    color_ratio = max(count1/count2, count1/count2)\n",
        "    return result_image, color_ratio\n",
        "\n",
        "\n",
        "def kmeans_segmentation(image):\n",
        "    # initial k-means segmentation\n",
        "    twoDimage = image.reshape((-1,3))\n",
        "    twoDimage = np.float32(twoDimage)\n",
        "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    ret,label,center=cv.kmeans(twoDimage,2,None,criteria,10,cv.KMEANS_PP_CENTERS)\n",
        "    center = np.uint8(center)\n",
        "    res = center[label.flatten()]\n",
        "    segmented_image = res.reshape((image.shape))\n",
        "    masked, color_ratio = mask_segmented_image(segmented_image)\n",
        "\n",
        "    # needs to be grayscale for contour and morphology\n",
        "    masked = cv.cvtColor(masked, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Avoid excessive filtering when necessary\n",
        "    if color_ratio > 2.0:\n",
        "        return masked\n",
        "\n",
        "    # Remove small contours\n",
        "    contours = cv.findContours(masked, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
        "    contours = contours[0] if len(contours) == 2 else contours[1] # unpack if tuple\n",
        "\n",
        "    largest_area = max(map(cv.contourArea, contours), default=0)\n",
        "    for c in contours:\n",
        "        area = cv.contourArea(c)\n",
        "        if area < largest_area:\n",
        "            cv.drawContours(masked, [c], -1, (0,0,0), -1)\n",
        "    # Morph close\n",
        "    kernel = cv.getStructuringElement(cv.MORPH_RECT, (5,5))\n",
        "    close = cv.morphologyEx(masked, cv.MORPH_CLOSE, kernel, iterations=2)\n",
        "    return close\n",
        "\n",
        "\n",
        "def binary_slic_segmentation(image):\n",
        "    segments = slic(image, n_segments=300, compactness=10, sigma=1)\n",
        "    return segments\n",
        "\n",
        "\n",
        "def classical_segmentation(image):\n",
        "    kmeans_mask = kmeans_segmentation(image)\n",
        "    superpixels = binary_slic_segmentation(image)\n",
        "    binary_mask = np.zeros_like(kmeans_mask)\n",
        "    for superpixel in np.unique(superpixels):\n",
        "        mask = np.zeros_like(kmeans_mask)\n",
        "        mask[superpixels == superpixel] = 255\n",
        "        overlap = np.logical_and(kmeans_mask, mask)*255\n",
        "        if np.sum(overlap) / np.sum(mask) > 0.5:\n",
        "            binary_mask[superpixels == superpixel] = 255\n",
        "    return binary_mask\n",
        "\n",
        "\n",
        "def segment_image(image, mode=0, model=None):\n",
        "    result_image = None\n",
        "    if mode == 0:\n",
        "        mask = classical_segmentation(image)\n",
        "        result_image = cv.cvtColor(mask, cv.COLOR_GRAY2BGR)\n",
        "    elif mode == 1:\n",
        "        dim = image.shape\n",
        "        image = cv.resize(image, (128, 128))\n",
        "        output = model.forward(image)\n",
        "        predicted_mask = (output != 0).float()\n",
        "        predicted_mask_numpy = predicted_mask.squeeze().cpu().numpy()*255.0\n",
        "        result_image = cv.resize(predicted_mask_numpy, (dim[1], dim[0]))\n",
        "    else:\n",
        "        print(\"Invalid mode\")\n",
        "    return result_image\n",
        "\n",
        "\n",
        "def segment_video(video_path, output_path):\n",
        "    cap = cv.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv.VideoWriter(output_path, fourcc, 2.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        segmented_image = segment_image(frame, mode=0)\n",
        "        out.write(segmented_image)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Segment the video files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in video_files:\n",
        "    video_path = folder + '/' + f\n",
        "    output_path = target_folder + '/' +  f\n",
        "    print(f\"Processing {f}\")\n",
        "    print(f\"Video Path: {video_path}\")\n",
        "    print(f\"Output Path: {output_path}\")\n",
        "    segment_video(video_path, output_path)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XdShrW_0M27"
      },
      "source": [
        "# Data Collection\n",
        "A Kaggle dataset (URL listed) was used to train the model. Follow the instructions at the listed link to find your kaggle token (kaggle.json), and upload it to the follwing cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "o_z4BVYGw8h5",
        "outputId": "4d7a8e59-47b2-43d9-d6f0-6ce4120832d3"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/discussions/general/74235\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwzqPLEa-Ki"
      },
      "source": [
        "Once the file is uploaded, run the following cell to download the dataset. If using a different dataset, the image and mask directories will likely need to be changed. The dataset used includes two directories, each containing entirely JPG files. One contains images of rivers and the other contains masks in which the river is white and everything else is black."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSATUkYau52Z",
        "outputId": "d87c7626-ca8f-479e-f817-a80ceec7679b"
      },
      "outputs": [],
      "source": [
        "import kaggle\n",
        "import os\n",
        "\n",
        "# Authenticate with Kaggle API\n",
        "kaggle.api.authenticate()\n",
        "\n",
        "# Download the dataset\n",
        "kaggle.api.dataset_download_files('franciscoescobar/satellite-images-of-water-bodies', path='./data', unzip=True)\n",
        "\n",
        "# Define the paths\n",
        "data_dir = '/content/data/Water Bodies Dataset/'\n",
        "image_dir = os.path.join(data_dir, 'Images')\n",
        "mask_dir = os.path.join(data_dir, 'Masks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G021ax8z0WHR"
      },
      "source": [
        "# Data Preprocessing\n",
        "A Dataset class is used to make the training easier to adapt to different datasets because it creates a uniform interface for the training process to access while abstracting away the details of the files storage and any simple transforms applied to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Q6z3rrq6V5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the paths\n",
        "data_dir = '/content/data/Water Bodies Dataset/'\n",
        "image_dir = os.path.join(data_dir, 'Images')\n",
        "mask_dir = os.path.join(data_dir, 'Masks')\n",
        "\n",
        "class RiverDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform, target_transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_base_name = os.path.splitext(img_name)[0]\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, img_base_name + '.jpg')\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        image = self.transform(image)\n",
        "        mask = self.target_transform(mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN5FQ0lQcbv_"
      },
      "source": [
        "The only transforms applied at this step are resizing all images to a uniform size (128 for computational limitations) and conversion to tensors. This transform is applied to both the images and the mask images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDZkGtx8x2iL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Define the transformations\n",
        "SIZE = 128\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Resize((SIZE, SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create the dataset and data loader\n",
        "dataset = RiverDataset(image_dir, mask_dir, target_transform, target_transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-yYZODBc2_p"
      },
      "source": [
        "The data was split into 80% training data and 20% testing data, each being given their own dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ymALKLYFrep"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "# Create data loaders for train and test sets\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gHRxJSqXJ-U"
      },
      "source": [
        "# Deep Learning Training\n",
        "The following model (A SwinV2 image transformer used as an encoder hooked up to a convolutional decoder) was found to be ineffective, as even with testing of various learning rates the extrema achieved by simply predicting a monochromatic mask (either 'all river' or all 'not river') which provided an approximately 70% accuracy from which the model did not improve at all over 10 epochs. Code is included to allow training over multiple runs of the cell by checking for uploaded model weights and using them. This also means that if one wants to run the training locally on their device in a way that supports more cores or GPU usage, they can then upload the weights to the notebook and include in the pipeline outlined here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyB9TYZNGa0c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoImageProcessor, Swinv2Model\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set the device (GPU or CPU)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = nn.ConvTranspose2d(768, 512, kernel_size=4, stride=4)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.conv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=4)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.conv3 = nn.ConvTranspose2d(128, 1, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 768, 8, 8)\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "\n",
        "class SemanticSegmentationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SemanticSegmentationModel, self).__init__()\n",
        "        self.image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
        "        self.encoder = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_processor(images=x, return_tensors=\"pt\")\n",
        "        encoder_output = self.encoder(**x)\n",
        "        last_hidden_state = encoder_output.last_hidden_state\n",
        "        x = self.decoder(last_hidden_state)\n",
        "        return x\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "model = SemanticSegmentationModel()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        image, mask = batch\n",
        "        image = image.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(image)\n",
        "        loss = criterion(output, mask)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
        "    torch.save(model.encoder.state_dict(), f\"encoder_epoch_{epoch+1}.pth\")\n",
        "    torch.save(model.decoder.state_dict(), f\"decoder_epoch_{epoch+1}.pth\")\n",
        "    print(f\"Model saved after epoch {epoch+1}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
